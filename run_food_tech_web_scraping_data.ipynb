{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_food_tech_web_scraping_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmulugheta/Introduction-to-Web-Scraping/blob/main/run_food_tech_web_scraping_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ftm_aQHPWlB"
      },
      "source": [
        "####Connect to gdrive (after you add a short to the share folder for FoodTech project)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9KOGK6PNFP",
        "outputId": "dddac0d8-0cab-428a-919a-e88695c67d52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJxCRrOGRbUE"
      },
      "source": [
        "#### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYY_Pi_-RddN",
        "outputId": "54abf712-274c-4a01-a871-80d55c9792f0"
      },
      "source": [
        "!pip install Scrapy\n",
        "!pip install crochet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Scrapy\n",
            "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 254 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 42.8 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting protego>=0.1.15\n",
            "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 32.0 MB/s \n",
            "\u001b[?25hCollecting Twisted[http2]>=17.9.0\n",
            "  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 47.3 MB/s \n",
            "\u001b[?25hCollecting h2<4.0,>=3.0\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from Scrapy) (4.2.6)\n",
            "Collecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->Scrapy) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.20)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->Scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (21.2.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->Scrapy) (3.7.4.3)\n",
            "Collecting priority<2.0,>=1.1.0\n",
            "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->Scrapy) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->Scrapy) (57.4.0)\n",
            "Building wheels for collected packages: protego, PyDispatcher\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7782 sha256=fd454bc0d252e7f91f4a363444f3a194862875633caafd87be870cd6d9f31bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=f8a2e52a95767a04375c36f8867b16f234eaf005c0b5656873505ff345e43ffd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
            "Successfully built protego PyDispatcher\n",
            "Installing collected packages: zope.interface, w3lib, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, priority, parsel, jmespath, itemadapter, h2, cryptography, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, Scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Scrapy-2.5.1 Twisted-21.7.0 constantly-15.1.0 cryptography-35.0.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyOpenSSL-21.0.0 queuelib-1.6.2 service-identity-21.1.0 w3lib-1.22.0 zope.interface-5.4.0\n",
            "Collecting crochet\n",
            "  Downloading crochet-2.0.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.7/dist-packages (from crochet) (21.7.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from crochet) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (3.7.4.3)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (5.4.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.0.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (15.1.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (20.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (57.4.0)\n",
            "Installing collected packages: crochet\n",
            "Successfully installed crochet-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkqh7Et-P7vY"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAGkZ4IkOu1P"
      },
      "source": [
        "import os, sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJzHxxvfeKjQ"
      },
      "source": [
        "# Scrapy imports\n",
        "# scrape webpage\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "# text cleaning\n",
        "import re\n",
        "# Reactor restart\n",
        "from crochet import setup, wait_for"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGAFFMDNWPeW"
      },
      "source": [
        "#### Support functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P75LVLQWUez"
      },
      "source": [
        "class WebscrapeToCsv(scrapy.Spider):\n",
        "    \"\"\"scrape first line of  quotes from website and save to json file\"\"\"\n",
        "    name = \"WebsiteToCsv\"\n",
        "    # start_urls = [\n",
        "    #     'https://www.colruyt.be/',\n",
        "    # ]\n",
        "    start_urls = [\n",
        "                  \"https://www.aldi.be/\"\n",
        "    ]\n",
        "    custom_settings = {\n",
        "          'ITEM_PIPELINES': {\n",
        "              '__main__.ExtractFirstLine': 1\n",
        "          },\n",
        "          'FEEDS': {\n",
        "              \"output.csv\": {\n",
        "                  'format': 'csv',\n",
        "                  'overwrite': True\n",
        "              }\n",
        "          }\n",
        "          # 'FEEDS': {\n",
        "          #     'promo_data.csv': {\n",
        "          #         'format': 'csv',\n",
        "          #         'overwrite': True\n",
        "          #     }\n",
        "          # }\n",
        "      }\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"parse data from urls\"\"\"\n",
        "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
        "            yield {'quote': quote.extract()}\n",
        "\n",
        "\n",
        "class ExtractFirstLine(object):\n",
        "    def process_item(self, item, spider):\n",
        "        \"\"\"text processing\"\"\"\n",
        "        lines = dict(item)[0].splitlines()\n",
        "        first_line = self.__remove_html_tags__(lines[0])\n",
        "\n",
        "        return {'0 - item': first_line}\n",
        "\n",
        "    def __remove_html_tags__(self, text):\n",
        "        \"\"\"remove html tags from string\"\"\"\n",
        "        html_tags = re.compile('<.*?>')\n",
        "        return re.sub(html_tags, '', text)\n",
        "\n",
        "@wait_for(10)\n",
        "def run_spider(start_urls, output_path):\n",
        "    \"\"\"run spider with WebscrapeToCsv\"\"\"\n",
        "    output_path = \"output.csv\"\n",
        "    # call setup - only once\n",
        "    setup()\n",
        "\n",
        "    crawler = CrawlerRunner()\n",
        "    d = crawler.crawl(WebscrapeToCsv)\n",
        "    return d\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9XBMA4X7A5"
      },
      "source": [
        "#### Set path and get websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmchfE2YLx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "70530f81-4c49-46f4-8897-35c7cf0dafca"
      },
      "source": [
        "current_run_path = (\"https://drive.google.com/drive/u/0/folders/1VgQL3PVyF4ix_CAScRbk6jLfHydVNtL9\")\n",
        "os.chdir(current_run_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b8d93925316b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcurrent_run_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"https://drive.google.com/drive/u/0/folders/1VgQL3PVyF4ix_CAScRbk6jLfHydVNtL9\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_run_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://drive.google.com/drive/u/0/folders/1VgQL3PVyF4ix_CAScRbk6jLfHydVNtL9'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFgwMRfYd5L"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/Official_Folder_Omdena_FoodTech\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-a63t0sX5Uj"
      },
      "source": [
        "websites_filename = os.path.join(base_path, \"3_Data\", \"Retail Websites for scrapping.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJ7mdbgXVKf"
      },
      "source": [
        "# load data and extract only links for now\n",
        "import chardet \n",
        "def get_char_encoding(path):\n",
        "  charenc = \"\"\n",
        "  rawdata = open(path, 'rb').read()\n",
        "  result = chardet.detect(rawdata)\n",
        "  charenc = result['encoding']\n",
        "  print(charenc)\n",
        "  return charenc\n",
        "\n",
        "def read_data(path):\n",
        "  df = None\n",
        "  char_enc = get_char_encoding(path)\n",
        "  with open(path, \"r\") as fs:\n",
        "    df = pd.read_csv(path, encoding=char_enc)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "E1eHQ_4EcYg7",
        "outputId": "7713a4f7-c583-47e3-ef23-2d1944fd2da3"
      },
      "source": [
        "df_websites = read_data(websites_filename)\n",
        "df_websites"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utf-8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>retailer</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Colruyt laagste prijzen</td>\n",
              "      <td>https://www.colruyt.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Okay</td>\n",
              "      <td>https://www.okay.be/okay/static/nl.shtml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bio planet</td>\n",
              "      <td>https://www.bioplanet.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alvo</td>\n",
              "      <td>https://www.alvo.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spar (Colruyt)</td>\n",
              "      <td>https://www.mijnspar.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Spar (Lambrechts)</td>\n",
              "      <td>https://www.spar.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Delhaize</td>\n",
              "      <td>https://www.delhaize.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Louis Delhaize</td>\n",
              "      <td>https://louisdelhaize.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Carrefour</td>\n",
              "      <td>https://www.carrefour.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Aldi</td>\n",
              "      <td>https://www.aldi.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Lidl</td>\n",
              "      <td>https://www.lidl.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Makro</td>\n",
              "      <td>https://www.makro.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Metro</td>\n",
              "      <td>https://www.metro.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Färm</td>\n",
              "      <td>https://www.farm.coop/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Jumbo</td>\n",
              "      <td>https://www.jumbo.com/nl-be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Albert Heijn</td>\n",
              "      <td>https://www.ah.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Intermarché</td>\n",
              "      <td>https://www.intermarche.be/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Match and smatch</td>\n",
              "      <td>https://www.supermarche-match.be/</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   retailer                                      link\n",
              "0   Colruyt laagste prijzen                   https://www.colruyt.be/\n",
              "1                      Okay  https://www.okay.be/okay/static/nl.shtml\n",
              "2                Bio planet                 https://www.bioplanet.be/\n",
              "3                      Alvo                      https://www.alvo.be/\n",
              "4            Spar (Colruyt)                  https://www.mijnspar.be/\n",
              "5         Spar (Lambrechts)                      https://www.spar.be/\n",
              "6                  Delhaize                  https://www.delhaize.be/\n",
              "7            Louis Delhaize                 https://louisdelhaize.be/\n",
              "8                 Carrefour                 https://www.carrefour.be/\n",
              "9                      Aldi                      https://www.aldi.be/\n",
              "10                     Lidl                      https://www.lidl.be/\n",
              "11                    Makro                     https://www.makro.be/\n",
              "12                    Metro                     https://www.metro.be/\n",
              "13                     Färm                    https://www.farm.coop/\n",
              "14                    Jumbo               https://www.jumbo.com/nl-be\n",
              "15             Albert Heijn                        https://www.ah.be/\n",
              "16              Intermarché               https://www.intermarche.be/\n",
              "17         Match and smatch         https://www.supermarche-match.be/"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy6KyM5nWxj8",
        "outputId": "f895950b-ab63-46c9-a522-66e9d82dd312"
      },
      "source": [
        "# start_urls = [\n",
        "#         'https://www.colruyt.be/',\n",
        "#     ]\n",
        "\n",
        "start_urls = df_websites[\"link\"]\n",
        "start_urls #[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                      https://www.colruyt.be/\n",
              "1     https://www.okay.be/okay/static/nl.shtml\n",
              "2                    https://www.bioplanet.be/\n",
              "3                         https://www.alvo.be/\n",
              "4                     https://www.mijnspar.be/\n",
              "5                         https://www.spar.be/\n",
              "6                     https://www.delhaize.be/\n",
              "7                    https://louisdelhaize.be/\n",
              "8                    https://www.carrefour.be/\n",
              "9                         https://www.aldi.be/\n",
              "10                        https://www.lidl.be/\n",
              "11                       https://www.makro.be/\n",
              "12                       https://www.metro.be/\n",
              "13                      https://www.farm.coop/\n",
              "14                 https://www.jumbo.com/nl-be\n",
              "15                          https://www.ah.be/\n",
              "16                 https://www.intermarche.be/\n",
              "17           https://www.supermarche-match.be/\n",
              "Name: link, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaawdiCqFe85"
      },
      "source": [
        "run_spider()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPze4-i8Gb0F",
        "outputId": "a8a49b8d-5e99-4812-f305-82b7a0ecc4ce"
      },
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgUhhQ8rQyRO",
        "outputId": "cfdb21a9-7f07-4093-b324-534c2205eb03"
      },
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [2 InRelease 47.5 kB/88.7 kB 54%] [Connecting to security.ubuntu.com (91.189\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [2 InRelease 47.5 kB/88.7 kB 54%] [Connecting to s\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [5 InRelease 9,844 B/74.6 kB 13%] [Connecting to s\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\u001b[0m\r                                                                               \rGet:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [6 InR\u001b[0m\r                                                                               \rGet:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,801 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n",
            "Get:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,365 kB]\n",
            "Fetched 11.9 MB in 3s (4,547 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 95.4 MB of archives.\n",
            "After this operation, 323 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 94.0.4606.71-0ubuntu0.18.04.1 [1,136 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 94.0.4606.71-0ubuntu0.18.04.1 [85.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 94.0.4606.71-0ubuntu0.18.04.1 [4,161 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 94.0.4606.71-0ubuntu0.18.04.1 [4,964 kB]\n",
            "Fetched 95.4 MB in 1s (67.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_94.0.4606.71-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANyu-U0dQ7Pq",
        "outputId": "942cd65b-2c63-47b4-b37c-6ec6f419c18f"
      },
      "source": [
        "!which chromedriver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/chromedriver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERipElbxQ76n",
        "outputId": "d91bae39-8f29-4159-afe6-38c9f0569b1b"
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 45.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 112 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 143 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 225 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 904 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW80h12qQkkn"
      },
      "source": [
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "prefs = {\n",
        "  \"translate_whitelists\": {\"ru\":\"en\"},\n",
        "  \"translate\":{\"enabled\":\"true\"}\n",
        "}\n",
        "\n",
        "options.add_experimental_option(\"prefs\", prefs)\n",
        "# open it, go to a website, and get results\n",
        "wd = webdriver.Chrome('chromedriver',options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojsNAOu0Kd9o"
      },
      "source": [
        "import bs4 as bs\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time        #note this line\n",
        "\n",
        "# url_link = 'https://www.instagram.com/explore/tags/モデル'\n",
        "url_link = start_urls[9]\n",
        "wd.get(url_link)\n",
        "\n",
        "time.sleep(8)                                          #note this as well moreover it should be after get method \n",
        "source = wd.execute_script(\"return document.body.innerHTML\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhU9ssxxT2JE"
      },
      "source": [
        "soup = bs.BeautifulSoup(source,'lxml')\n",
        "print(soup)\n",
        "out_path = \"/content/output\"\n",
        "if not os.path.exists(out_path):\n",
        "  os.makedirs(out_path)\n",
        "out_file = os.path.join(out_path, \"sample.html\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD8V795oSM48"
      },
      "source": [
        "# curr_content = soup.find('span', class_='g47SY ') #.text\n",
        "curr_content = soup.find('div', class_=\"mod-stage-theme__media mod-stage-theme__media--picture\")\n",
        "print(curr_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcJys8gRrVDJ"
      },
      "source": [
        "def extract_elems_by_class(curr_soup, elem, cls):\n",
        "  extracted_elems = []\n",
        "  try:\n",
        "    extracted_elems = curr_soup.find_all(elem, class_=cls)\n",
        "  except Exception as ee:\n",
        "    print(f\"Unable to extract for element: {elem} with class: {cls}\")\n",
        "\n",
        "  return extracted_elems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtf2U_EUjXBE",
        "outputId": "fef0a867-1319-46fa-d7a1-6eb47c8b1266"
      },
      "source": [
        "# for class=\"mod-offer-category__content\"\n",
        "curr_class=\"mod-offer-category__content\"\n",
        "html_elem = \"div\"\n",
        "# promo_categories = soup.find_all('div', class_=curr_class)\n",
        "promo_categories = extract_elems_by_class(soup, html_elem, curr_class)\n",
        "print(promo_categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Thema-actie Frankrijk</h3>\n",
            "<span class=\"mod-offer-category__tagline\">sinds wo 06/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Merkenpromo</h3>\n",
            "<span class=\"mod-offer-category__tagline\">sinds za 09/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Verspromo</h3>\n",
            "<span class=\"mod-offer-category__tagline\">sinds ma 11/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Promo food &amp; non-food</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf vr 15/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Food &amp; non-food</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf vr 15/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Badkamer</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf za 16/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Multimedia</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf za 16/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Verspromo</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf ma 18/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Kleding</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf wo 20/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Overige</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf wo 20/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Promo food &amp; non-food</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf vr 22/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Food &amp; non-food</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf vr 22/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Kleding</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf za 23/10</span>\n",
            "</div>, <div class=\"mod-offer-category__content\">\n",
            "<h3 class=\"mod-offer-category__title\">Interieur</h3>\n",
            "<span class=\"mod-offer-category__tagline\">vanaf za 23/10</span>\n",
            "</div>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhth5tidkTvi"
      },
      "source": [
        "import lxml.html as lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Dm6_lgpW5E",
        "outputId": "2aea3b66-d8d0-4d27-814b-724b2629a3eb"
      },
      "source": [
        "promo_categories[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<div class=\"mod-offer-category__content\">\n",
              "<h3 class=\"mod-offer-category__title\">Thema-actie Frankrijk</h3>\n",
              "<span class=\"mod-offer-category__tagline\">sinds wo 06/10</span>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7SwZSEZkVaR",
        "outputId": "96be95ca-d563-4c27-a5c3-9bf0f038c832"
      },
      "source": [
        "# tr_elements = promo_categories.xpath('//h3')\n",
        "# doc = lh.fromstring(page.content)\n",
        "# extracted_elements = [(lh.fromstring(item)).xpath('//h3') for item in promo_categories]\n",
        "extracted_elements = [item for item in promo_categories]\n",
        "# print(extracted_elements)\n",
        "curr_class = \"mod-offer-category__title\"\n",
        "# extracted_cats = soup.find_all('h3', class_=curr_class)\n",
        "html_elem = \"h3\"\n",
        "extracted_cats = extract_elems_by_class(soup, html_elem, curr_class)\n",
        "promo_categories = [item.text for item in extracted_cats]\n",
        "print(promo_categories)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thema-actie Frankrijk', 'Merkenpromo', 'Verspromo', 'Promo food & non-food', 'Food & non-food', 'Badkamer', 'Multimedia', 'Verspromo', 'Kleding', 'Overige', 'Promo food & non-food', 'Food & non-food', 'Kleding', 'Interieur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHZJg8S4RrU7",
        "outputId": "13dafd0e-b303-455c-9257-6aaaff15d8d1"
      },
      "source": [
        "open(out_file, 'w').write(wd.page_source)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1907628"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}